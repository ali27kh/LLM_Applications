# ğŸ“Š LangChain: Evaluation Framework

## ğŸ”‘ Keywords
- **LangChain**
- **LLM Evaluation**
- **RetrievalQA**
- **CSVLoader**
- **DocArrayInMemorySearch**
- **QAGenerateChain**
- **QAEvalChain**
- **ROUGE / F1 / Semantic Similarity**
- **LLM-assisted Evaluation**

---

## ğŸ“– Overview
This project demonstrates how to **evaluate a LangChain QA system** using multiple methods:

1. **Example Generation** (hard-coded + LLM-generated).
2. **Manual Evaluation** with debugging mode.
3. **LLM-Assisted Evaluation** with `QAEvalChain`.
4. **Extended Performance Metrics**:
   - âœ… Exact Match
   - âœ… F1 Score
   - âœ… ROUGE-L
   - âœ… Semantic Similarity (cosine similarity of embeddings)

The evaluation shows how **strict lexical metrics** (Exact Match, ROUGE) may underestimate performance, while **semantic similarity** better reflects LLM quality.

---

## âš™ï¸ Workflow

1. **Setup**
   - Load environment variables (`.env`) with `dotenv`.
   - Handle **model versioning** depending on the date (`gpt-3.5-turbo` vs `gpt-3.5-turbo-0301`).

2. **QA Application**
   - Load CSV file with `CSVLoader`.
   - Create vector index with `DocArrayInMemorySearch`.
   - Build `RetrievalQA` chain using **OpenAI Chat model**.

3. **Examples Creation**
   - Add **hardcoded QA pairs**.
   - Use `QAGenerateChain` to generate new examples from dataset.

4. **Evaluation**
   - **Manual Evaluation** with debug mode.
   - **LLM-Assisted Evaluation** using `QAEvalChain`.
   - **Custom Metrics**:
     - Exact Match
     - F1 Score
     - ROUGE-L
     - Semantic Similarity

---

## ğŸš€ Example Usage

### Hardcoded Examples
```python
examples = [
    {
        "query": "Do the Cozy Comfort Pullover Set have side pockets?",
        "answer": "Yes"
    },
    {
        "query": "What collection is the Ultra-Lofty 850 Stretch Down Hooded Jacket from?",
        "answer": "The DownTek collection"
    }
]
```

### âœ… LLM-Assisted Evaluation
```python
from langchain.evaluation.qa import QAEvalChain

llm = ChatOpenAI(temperature=0, model=llm_model)
eval_chain = QAEvalChain.from_llm(llm)

graded_outputs = eval_chain.evaluate(examples, predictions)
```

### âœ… Extended Metrics
```python
results = evaluate(examples, predictions)
print(results)
```

### ğŸ“Š Sample Results
- **Exact Match**: 0.14 â†’ Only 14% of answers are strictly identical.
- **F1 Score**: 0.52
- **ROUGE-L**: 0.54
- **Semantic Similarity**: 0.91 âœ…

#### ğŸ‘‰ Interpretation:
- LLM answers are semantically accurate but not always lexically identical.
- Semantic Similarity confirms strong performance despite low token-based scores.

---

## âœ… Features
- ğŸ“‚ Easy CSV ingestion for QA tasks
- ğŸ” Evaluation pipeline with manual + automated grading
- ğŸ“Š Rich metrics for text evaluation
- ğŸ”„ Extendable to other datasets and evaluation strategies